{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM1JWMZQU_rx"
      },
      "source": [
        " Task 1: Tabular Q-Learning Update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Task 1.1- Q-table initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc2YaIgaUPrb"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LREYLZCPxa1_",
        "outputId": "95dbee47-ceec-48b7-eb9a-a2bcf6e754ee"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.23.5\n",
        "#ignore.It is needed for task 5. Was done while debugging task 5 due to gym and numpy versions compatibility error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pth0QgGeVJBw"
      },
      "outputs": [],
      "source": [
        "def init_q_table(n_states,n_actions) :\n",
        " return np.zeros((n_states,n_actions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4oND5HV57V"
      },
      "source": [
        "Task 1.2- Q-table update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prpXJ_1fWNrB"
      },
      "outputs": [],
      "source": [
        " def q_update(Q, s, a, r, s_next, alpha, gamma):\n",
        "    best_next_q = np.max(Q[s_next])          # max_a′ Q[s_next, a′]\n",
        "    td_target = r + gamma * best_next_q      # target value\n",
        "    td_error = td_target - Q[s, a]           # TD error\n",
        "    Q[s, a] += alpha * td_error              # Q-learning update\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX3T6xYb1pOi"
      },
      "source": [
        "Task 2: ε-Greedy Policy on a Custom GridWorld\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmvdbeK81p3J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.n_states = size * size\n",
        "        self.n_actions = 4  # 0=up, 1=down, 2=left, 3=right\n",
        "        self.terminal_states = [0, self.n_states - 1]\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 1  # Start from a non-terminal state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(self.state, self.size)\n",
        "\n",
        "        if self.state in self.terminal_states:\n",
        "            return self.state, 0, True  # No move from terminal\n",
        "\n",
        "        # Move based on action\n",
        "        if action == 0 and row > 0:                # Up\n",
        "            row -= 1\n",
        "        elif action == 1 and row < self.size - 1:  # Down\n",
        "            row += 1\n",
        "        elif action == 2 and col > 0:              # Left\n",
        "            col -= 1\n",
        "        elif action == 3 and col < self.size - 1:  # Right\n",
        "            col += 1\n",
        "\n",
        "        new_state = row * self.size + col\n",
        "        reward = 0 if new_state in self.terminal_states else -1\n",
        "        done = new_state in self.terminal_states\n",
        "        self.state = new_state\n",
        "        return new_state, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.full((self.size, self.size), '.', dtype=str)\n",
        "        r, c = divmod(self.state, self.size)\n",
        "        grid[r, c] = 'A'\n",
        "        for t in self.terminal_states:\n",
        "            tr, tc = divmod(t, self.size)\n",
        "            grid[tr, tc] = 'T'\n",
        "        print('\\n'.join(' '.join(row) for row in grid))\n",
        "        print()\n",
        "\n",
        "def select_action(Q, state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(Q.shape[1])  # explore\n",
        "    return np.argmax(Q[state])  # exploit\n",
        "\n",
        "def q_update(Q, s, a, r, s_next, alpha, gamma):\n",
        "    best_next_q = np.max(Q[s_next])\n",
        "    td_target = r + gamma * best_next_q\n",
        "    Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "    return Q\n",
        "\n",
        "def train_agent(epsilon):\n",
        " env = GridWorld()\n",
        " n_states = env.n_states\n",
        " n_actions = env.n_actions\n",
        "\n",
        " Q = np.zeros((n_states, n_actions))  # Initialize Q-table\n",
        "\n",
        " # Hyperparameters\n",
        " alpha = 0.1        # Learning rate\n",
        " gamma = 0.99       # Discount factor\n",
        " episodes = 500     # Number of episodes\n",
        " reward_log = []  # To store total reward per episode\n",
        " for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0  # Reset total reward\n",
        "\n",
        "\n",
        "    while not done:\n",
        "        action = select_action(Q, state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        Q = q_update(Q, state, action, reward, next_state, alpha, gamma)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    reward_log.append(total_reward)  # Store episode reward\n",
        " return reward_log\n",
        "\n",
        "\n",
        "rewards_eps_01 = train_agent(0.1)\n",
        "rewards_eps_02 = train_agent(0.2)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGUXMEz2XHZq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def moving_average(data, window_size=50):\n",
        "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "# Assume these are your reward logs from training with different epsilons\n",
        "# rewards_eps_01 = train_agent(0.1)\n",
        "# rewards_eps_02 = train_agent(0.2)\n",
        "\n",
        "# Compute moving averages\n",
        "ma_rewards_01 = moving_average(rewards_eps_01, 50)\n",
        "ma_rewards_02 = moving_average(rewards_eps_02, 50)\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.plot(range(len(ma_rewards_01)), ma_rewards_01, label='ε=0.1')\n",
        "plt.plot(range(len(ma_rewards_02)), ma_rewards_02, label='ε=0.2')\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Moving Average Reward (window=50)')\n",
        "plt.title('Moving Average Reward vs Episodes for Different ε')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcIN7I8xxHp_"
      },
      "source": [
        "Task 3: Experience Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtdhZ5hcX8wo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Replay Buffer Implementation:\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = np.stack(states).astype(np.float32)\n",
        "        actions = np.array(actions, dtype=np.int64) # Actions are usually integers\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        next_states = np.stack(next_states).astype(np.float32)\n",
        "        dones = np.array(dones, dtype=np.bool_) # Dones are booleans\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        " #Create a ReplayBuffer instance with capacity 100\n",
        "\n",
        "# 1. Create a ReplayBuffer instance with capacity 100\n",
        "\n",
        "replay_buffer = ReplayBuffer(capacity=100)\n",
        "\n",
        "# 2. Fill the buffer with 100 random transitions\n",
        "for _ in range(100):\n",
        "    state = np.random.randint(0, 10, size=(4,))        # Example state: 4D integer vector\n",
        "    action = np.random.randint(0, 4)                   # Example action: integer 0–3\n",
        "    reward = np.random.uniform(-1, 1)                  # Example reward: float between -1 and 1\n",
        "    next_state = np.random.randint(0, 10, size=(4,))   # Example next state\n",
        "    done = np.random.choice([True, False])             # Done flag\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "# 3. Sample a batch of 32 transitions\n",
        "batch_size = 32\n",
        "states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "# 4. Print shapes and data types\n",
        "print(\"Sampled batch shapes and types:\")\n",
        "print(\"States:\",states.shape, states.dtype)\n",
        "print(\"Actions:\", actions.shape, actions.dtype)\n",
        "print(\"Rewards:\", rewards.shape, rewards.dtype)\n",
        "print(\"Next States:\", next_states.shape, next_states.dtype)\n",
        "print(\"Dones:\", dones.shape, dones.dtype)\n",
        "# Buffer enteries.\n",
        "\"\"\"\n",
        "def print_buffer(replay_buffer):\n",
        "    for i, (state, action, reward, next_state, done) in enumerate(replay_buffer.buffer):\n",
        "        print(f\"Transition #{i+1}\")\n",
        "        print(f\"  State      : {state}\")\n",
        "        print(f\"  Action     : {action}\")\n",
        "        print(f\"  Reward     : {reward}\")\n",
        "        print(f\"  Next State : {next_state}\")\n",
        "        print(f\"  Done       : {done}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Call it with your ReplayBuffer instance\n",
        "print_buffer(replay_buffer)\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJKojFpJz-EY"
      },
      "source": [
        "Task 4: Deep Q-Network with Target Copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CEnCa-eyJNq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQNPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions):\n",
        "        super(DQNPolicy, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# DQNTarget is identical to DQNPolicy\n",
        "class DQNTarget(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions):\n",
        "        super(DQNTarget, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "def update_target(policy_net, target_net):\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtZQ1r_-j7Du"
      },
      "source": [
        "Task 5: Full DQN Training Loop on CartPole-v1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjdnMHWSWiSq"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "N = 1000              # Number of episodes\n",
        "M = N // 2            # ε decay episodes\n",
        "X = 10                # Target update frequency\n",
        "gamma = 0.99          # Discount factor\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "buffer_capacity = 10000\n",
        "\n",
        "# Initialize\n",
        "env = gym.make(\"CartPole-v1\",new_step_api=True)\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQNPolicy(obs_dim, n_actions)\n",
        "target_net = DQNTarget(obs_dim, n_actions)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.MSELoss()\n",
        "replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "episode_rewards = []\n",
        "\n",
        "# Training loop\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = (epsilon_start - epsilon_end) / M\n",
        "\n",
        "for episode in range(1, N + 1):\n",
        "    result = env.reset()\n",
        "    state = result[0] if isinstance(result, tuple) else result\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        epsilon = max(epsilon_end, epsilon_start - episode * epsilon_decay)\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "                q_values = policy_net(state_tensor)\n",
        "                action = q_values.argmax().item()\n",
        "\n",
        "        next_state, reward, terminated, truncated,info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        replay_buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(replay_buffer.buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "            states = torch.FloatTensor(states)\n",
        "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
        "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "            next_states = torch.FloatTensor(next_states)\n",
        "            dones = torch.FloatTensor(dones).unsqueeze(1)\n",
        "\n",
        "            # Q(s, a)\n",
        "            q_values = policy_net(states).gather(1, actions)\n",
        "\n",
        "            # max_a' Q_target(s', a')\n",
        "            with torch.no_grad():\n",
        "                max_next_q_values = target_net(next_states).max(dim=1, keepdim=True)[0]\n",
        "                target_q_values = rewards + gamma * max_next_q_values * (1 - dones)\n",
        "\n",
        "            loss = loss_fn(q_values, target_q_values)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    if episode % X == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    if episode % (N // 10) == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-(N // 10):])\n",
        "        print(f\"Episode {episode}, Avg reward over last {N // 10}: {avg_reward:.2f}\")\n",
        "\n",
        "# Plotting\n",
        "window = N // 10\n",
        "moving_avg = [np.mean(episode_rewards[max(0, i-window):i+1]) for i in range(N)]\n",
        "\n",
        "plt.plot(range(N), episode_rewards, label='Reward')\n",
        "plt.plot(range(N), moving_avg, label='Moving Avg', linewidth=2)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"DQN Training on CartPole\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Final success rate\n",
        "success_count = sum(r >= 195 for r in episode_rewards)\n",
        "success_rate = 100 * success_count / N\n",
        "print(f\"Success Rate: {success_rate:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
